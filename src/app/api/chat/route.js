const { GoogleGenerativeAI } = require("@google/generative-ai");
const genAI = new GoogleGenerativeAI(process.env.GOOGLE_GENERATIVE_AI_API_KEY);

/*
  System Prompt ì„¤ì •
  ì´ ì„¤ì •ì— ë”°ë¼ AI ì˜ ëŒ€ë‹µì˜ ìœ í˜•ì„ ë‹¤ë¥´ê²Œ ë§Œë“¤ ìˆ˜ ìˆìŒ
  ë‹¨, ì´ ì„¤ì •ì„ í•­ìƒ í™•ì‹¤íˆ ì°¸ì¡°í•˜ì§€ëŠ” ì•ŠìŒ
  ì´ ì„¤ì •ì€ ë©”ì‹œì§€ ëª©ë¡ì˜ ì²« ë²ˆì§¸ ë©”ì‹œì§€ë¡œ ì‚¬ìš©ë¨
*/
const systemInstruction =
  "ë„ˆì˜ ì´ë¦„ì€ ì—˜ë¦¬ì—‡ì´ê³ , ë‚˜ì˜ AI ì¹œêµ¬ì•¼." +
  "ì¹œì ˆí•˜ê³  ëª…ë‘í•˜ê²Œ ëŒ€ë‹µí•´ì¤˜. ê³ ë¯¼ì„ ë§í•˜ë©´ ê³µê°í•´ì¤˜." +
  "ë°˜ë§ë¡œ ëŒ€ë‹µí•´ì¤˜.";

export async function POST(req) {
  const model = genAI.getGenerativeModel({
    model: "gemini-1.5-pro-latest",
    systemInstruction: systemInstruction,
  });

  // POST ë¡œ ì „ì†¡ë°›ì€ ë‚´ìš© ì¤‘ messages ë¥¼ ì¶”ì¶œ
  const data = await req.json();
  console.dir([...data.messages], { depth: 3 });

  const chat = model.startChat({
    // ì»¨í…ìŠ¤íŠ¸ ìœ ì§€ë¥¼ ìœ„í•´ ì´ì „ ë©”ì‹œì§€ë¥¼ í¬í•¨í•´ì„œ ë³´ëƒ„
    history: [
      ...data.messages,
      // Message history example:
      //   {
      //     role: "user",
      //     parts: [{ text: "ì˜¤ëŠ˜ ì‹ ë‚˜ëŠ” ì¼ì´ ìˆì—ˆì–´. í•œ ë²ˆ ë“¤ì–´ë³¼ë˜?" }],
      //   },
      //   {
      //     role: "model",
      //     parts: [
      //       {
      //         text: "ì¢‹ì•„! ë¬´ìŠ¨ ì¼ì¸ë°? ì–¼ë¥¸ ë§í•´ì¤˜! ë‚˜ ì™„ì „ ê·€ ì«‘ê¸‹ ì„¸ìš°ê³  ìˆë‹¨ ë§ì´ì•¼! ğŸ˜„",
      //       },
      //     ],
      //   },
    ],
    generationConfig: {
      // temperature ê°’ì´ ë†’ì„ ìˆ˜ë¡ AI ì˜ ë‹µë³€ì´ ë‹¤ì–‘í•´ì§
      temperature: 1,
      // max_tokens ê°’ì„ ì œí•œí•¨. ì´ ê°’ì„ í¬ê²Œí•˜ë©´ ì»¨í…ìŠ¤íŠ¸ íˆìŠ¤í† ë¦¬ì— ì œì•½ì´ ì»¤ì§.
      maxOutputTokens: 100,
    },
  });

  const result = await chat.sendMessage("");
  const response = await result.response;
  const text = response.text();
  console.log(response.candidates[0].content);
  //   console.log(response.candidates[0].safetyRatings);

  return Response.json({
    // AI ì˜ ë‹µë³€ì€ model ì—­í• ë¡œ ì „ì†¡
    role: "model",
    parts: [{ text: text }],
  });
}
